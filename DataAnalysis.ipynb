{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mberkancetin/Customizable-Structured-Electronic-Brainwriting/blob/main/DataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB2FpapKAkd5"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/mberkancetin/Customizable-Structured-Electronic-Brainwriting/blob/main/images/CSEB-logo.png?raw=true\" width=\"70\" style=\"vertical-align: middle;\">\n",
        "</p>\n",
        "\n",
        "# CSEB/mCSEB Semantic Analysis Suite: From Text to Topological Structure\n",
        "\n",
        "**Author:** <input type=\"text\" id=\"author\" value=\"Your Name or Research Group\" size=\"40\">\n",
        "\n",
        "<script>\n",
        "  document.getElementById(\"author\").addEventListener(\"input\", function() {\n",
        "    console.log(\"Author set to:\", this.value);\n",
        "  });\n",
        "</script>\n",
        "\n",
        "**Methodology:** Natural Language Processing (NLP) & Unsupervised Machine Learning  \n",
        "\n",
        "## Overview\n",
        "This notebook implements a rigorous computational pipeline to analyze qualitative data generated from Customizable Structured Electronic Brainwriting (CSEB) and Multilingual CSEB (mCSEB) sessions. Unlike traditional qualitative coding which relies solely on human interpretation, this workflow utilizes high-dimensional vector space modeling to validate semantic structures empirically.\n",
        "\n",
        "## Methodological Framework\n",
        "1.  **Text Preprocessing:** Lemmatization and domain-specific synonym mapping using `spaCy`.\n",
        "2.  **Vectorization:** Generating semantic embeddings using Sentence-BERT (all-MiniLM-L6-v2) to capture contextual meaning rather than lexical frequency (Reimers & Gurevych, 2019).\n",
        "\n",
        "3. **Dimensionality Reduction (UMAP):** Projecting high-dimensional vectors into 2D space to visualize the semantic topology and local manifolds (McInnes et al., 2018).\n",
        "\n",
        "4. **Structural Validation (K-Means & Centroids):** Comparing machine-generated clusters against human-coded criteria using Silhouette Scores and Cosine Similarity to verify taxonomic distinctness.\n",
        "\n",
        "5. **Network Analysis:** Constructing a keyword co-occurrence graph using Louvain Community Detection to identify latent thematic bridges (Blondel et al., 2008).\n",
        "\n",
        "6. **Advanced Topic Modeling (BERTopic):** Extracting coherent semantic topics using a class-based TF-IDF (c-TF-IDF) procedure to characterize the unsupervised clusters (Grootendorst, 2022).\n",
        "\n",
        "7. **Efficient Classification (SetFit):** Operationalizing the coding scheme with minimal labeled data using Contrastive Learning and sentence transformer fine-tuning (Tunstall et al., 2022)."
      ],
      "id": "YB2FpapKAkd5"
    },
    {
      "cell_type": "markdown",
      "id": "9b116462",
      "metadata": {
        "id": "9b116462"
      },
      "source": [
        "## 1. Environment Setup & Library Installation\n",
        "This cell installs the necessary libraries for NLP, Machine Learning, and Visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiiQJ7W-Akd8"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
        "!pip install -q umap-learn sentence-transformers\n",
        "!pip install -q spacy python-louvain networkx\n",
        "!pip install -q bertopic setfit datasets\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap\n",
        "import networkx as nx\n",
        "import spacy\n",
        "import torch\n",
        "from IPython.display import display\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, homogeneity_score, completeness_score, adjusted_mutual_info_score, classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from community import community_louvain\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from setfit import SetFitModel, Trainer\n",
        "from datasets import Dataset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# REPRODUCIBILITY CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "RANDOM_STATE = 42\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "seed_everything(RANDOM_STATE)\n",
        "print(\"✅ Libraries installed and environment seeded for reproducibility.\")"
      ],
      "id": "CiiQJ7W-Akd8"
    },
    {
      "cell_type": "markdown",
      "id": "f147a9d9",
      "metadata": {
        "id": "f147a9d9"
      },
      "source": [
        "## 2. Data Loading & Configuration\n",
        "Please paste below the Google Sheet ID, sheet name, and column names provided by the Apps Script pop-up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b882350a",
      "metadata": {
        "id": "b882350a"
      },
      "outputs": [],
      "source": [
        "# SETUP\n",
        "# ==== PASTE YOUR SESSION VARIABLES HERE =====\n",
        "spreadsheet_id = \"Please-paste-your-spreadsheet-ID\"\n",
        "sheet_name = \"PrepData\"\n",
        "ideaID = \"IdeaID\"\n",
        "idea_timestamp = \"IdeaTimestamp\"\n",
        "round_timestamp = \"RoundTimestamp\"\n",
        "original_column = \"RawIdea\"\n",
        "translate_column = \"Translation\"\n",
        "category_manual = \"ManualCategories\"\n",
        "# ==== PASTE YOUR SESSION VARIABLES ABOVE ====="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Categorization details\n",
        "Before running the analysis, the system needs to know which \"Tags\" or \"Labels\" to use for the charts. Please indicate in the following cell.\n",
        "\n",
        "\n",
        "\n",
        "*   Manual Categories: Check these boxes if you have already human-coded your data (e.g., you have columns for \"Main Criteria\" and \"Sub-Criteria\"). The analysis will then compare your human tags against the AI's findings.\n",
        "\n",
        "*   Automated Labeling: If you have not manually tagged your data (or want to ignore your tags), you must choose an AI method (K-Means or BERTopic) to generate categories for you automatically based on the text similarity.\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "To establish a dependent variable for the subsequent topological analysis, a definitive taxonomy must be selected. This configuration block enforces a strict hierarchy of data sources:\n",
        "\n",
        "\n",
        "\n",
        "*   Priori Coding (Manual): Prioritizes human-verified qualitative codes if available, enabling supervised validation metrics (e.g., Homogeneity Score).\n",
        "\n",
        "*   A Posteriori Clustering (Automated): In the absence of manual labels, the system defaults to an unsupervised clustering target. The user must select between Geometric Partitioning (K-Means) for fixed-cluster analysis or Density-Based Modeling (BERTopic) for dynamic topic extraction."
      ],
      "metadata": {
        "id": "NwRy8_sWiG3D"
      },
      "id": "NwRy8_sWiG3D"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Additional details for categorization\n",
        "#@markdown ⬇️ **Do you have manual categories that can be used in comparative analysis?**\n",
        "RUN_MANUAL_CAT = False #@param {type:\"boolean\"}\n",
        "if RUN_MANUAL_CAT:\n",
        "    CRITERIA_COL = \"criteria_code\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ⬇️ **Do you have sub_categories that can be used in comparative analysis?**\n",
        "RUN_SUB_CAT = False #@param {type:\"boolean\"}\n",
        "if RUN_SUB_CAT:\n",
        "    SUBCRITERIA_COL = \"subcriteria_code\" # @param {type:\"string\"}\n",
        "\n",
        "if not RUN_MANUAL_CAT and not RUN_SUB_CAT:\n",
        "  #@markdown ⬇️ **Automated Source: If no manual labels, how should we group the ideas?**\n",
        "  kmeans_labels = \"kmeans_labels\"\n",
        "  bertopic_labels = \"bertopic_labels\"\n",
        "  MACHINE_CAT = kmeans_labels # @param [\"kmeans_labels\", \"bertopic_labels\"] {\"type\":\"raw\"}\n"
      ],
      "metadata": {
        "id": "XHbgTQ5mThb7"
      },
      "id": "XHbgTQ5mThb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Google Sheets connection\n",
        "\n",
        "Google Sheets connection to read and write."
      ],
      "metadata": {
        "id": "hBXj2-iBjTxq"
      },
      "id": "hBXj2-iBjTxq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-dJnkbxAkd-"
      },
      "outputs": [],
      "source": [
        "# Authentication and setup\n",
        "def connect_to_sheets():\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    return gc\n",
        "\n",
        "# Load data from Google Sheets\n",
        "def load_ideas_from_sheets(spreadsheet_id, sheet_name):\n",
        "    gc = connect_to_sheets()\n",
        "    sh = gc.open_by_key(spreadsheet_id)\n",
        "    worksheet = sh.worksheet(sheet_name)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    headers = data[0]\n",
        "    ideas_data = data[1:]\n",
        "\n",
        "    df = pd.DataFrame(ideas_data, columns=headers)\n",
        "    df = df[df[translate_column] != \"#VALUE!\"]\n",
        "    df = df.dropna(subset=[translate_column]).reset_index()\n",
        "\n",
        "    return df\n",
        "\n",
        "def write_results_to_sheets(spreadsheet_id, processed_data, new_worksheet_name):\n",
        "    \"\"\"\n",
        "    Write processed data back to a new worksheet in the same Google Sheets file.\n",
        "\n",
        "    Parameters:\n",
        "    - spreadsheet_id: The ID of the Google Sheets document\n",
        "    - processed_data: DataFrame containing the processed results\n",
        "    - new_worksheet_name: Name of the new worksheet to create\n",
        "\n",
        "    Returns:\n",
        "    - The newly created worksheet object\n",
        "    write_results_to_sheets(spreadsheet_id, results_df, 'Processed_Results')\n",
        "    \"\"\"\n",
        "    gc = connect_to_sheets()\n",
        "    sh = gc.open_by_key(spreadsheet_id)\n",
        "\n",
        "    try:\n",
        "        existing_worksheet = sh.worksheet(new_worksheet_name)\n",
        "        sh.del_worksheet(existing_worksheet)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        pass\n",
        "\n",
        "    new_worksheet = sh.add_worksheet(title=new_worksheet_name, rows=processed_data.shape[0]+1, cols=processed_data.shape[1])\n",
        "    data_to_write = [processed_data.columns.tolist()] + processed_data.values.tolist()\n",
        "\n",
        "    # Write the data to the new worksheet\n",
        "    new_worksheet.update(range_name='A1', values=data_to_write)\n",
        "    print(f\"Successfully wrote results to new worksheet: {new_worksheet_name}\")\n",
        "\n",
        "    return new_worksheet"
      ],
      "id": "u-dJnkbxAkd-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb93fc3",
      "metadata": {
        "id": "5eb93fc3"
      },
      "outputs": [],
      "source": [
        "df = load_ideas_from_sheets(spreadsheet_id, sheet_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARXaBFkKAkd_"
      },
      "source": [
        "## 3. NLP Preprocessing Pipeline\n",
        "To ensure high-fidelity semantic analysis, we apply a two-stage preprocessing pipeline:\n",
        "1.  **Lemmatization:** Reducing inflectional forms (e.g., \"producing\", \"produced\") to their base lemma (\"product\") using the `en_core_web_sm` language model.\n",
        "2.  **Synonym Mapping:** Merging conceptually identical terms (e.g., \"consumer\" $\\to$ \"customer\", \"wage\" $\\to$ \"salary\") to reduce sparsity in the keyword network.\n",
        "\n",
        "**What is this step?**\n",
        "\n",
        "Before analyzing the data, we must clean it. Raw text is messy as it contains capitalization variations, plural forms (e.g., \"wage\" vs. \"wages\"), and filler words. This step standardizes the text so the computer treats \"worker,\" \"employee,\" and \"workforce\" as the same concept, ensuring our analysis focuses on meaning rather than spelling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "To reduce sparsity in the vector space, we apply a two-stage preprocessing pipeline using the spaCy library. First, Lemmatization reduces inflectional forms to their lexicographical root (e.g., producing $\\to$ product). Second, domain-specific Synonym Mapping consolidates semantically equivalent terms based on the specific ontology of the textile industry. This normalization minimizes noise and enhances the signal-to-noise ratio for downstream embedding tasks.\n",
        "\n",
        "**Reference:** Manning, C. D., Raghavan, P., & Schütze, H. (2009). *Introduction to Information Retrieval.* Cambridge University Press. Online Edition. https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf"
      ],
      "id": "ARXaBFkKAkd_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQvjgJ48Akd_"
      },
      "outputs": [],
      "source": [
        "# Please check https://spacy.io/usage/models if the analysis language is not English\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\") # Load Spacy Model\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Basic string cleaning.\"\"\"\n",
        "    text = str(text).lower().strip()\n",
        "    text = re.sub(r'\\s+', ' ', text) # Remove multi-space\n",
        "    text = re.sub(r'[^a-z0-9\\s\\-]', '', text) # Remove special chars\n",
        "    return text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Spacy lemmatization.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def map_synonyms(text):\n",
        "    words = text.split()\n",
        "    return \" \".join([synonym_map.get(word, word) for word in words])"
      ],
      "id": "hQvjgJ48Akd_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff600f0",
      "metadata": {
        "id": "bff600f0"
      },
      "outputs": [],
      "source": [
        "# Domain-Specific Synonym Map Example\n",
        "synonym_map = {\n",
        "    \"child labor\": \"child_labor\",\n",
        "    \"renewable energy\": \"renewable_energy\",\n",
        "    \"supply chain\": \"supply_chain\",\n",
        "    \"supply chains\": \"supply_chain\",\n",
        "    \"consumer\":\"customer\",\n",
        "    \"consumers\":\"customer\",\n",
        "    \"client\":\"client\",\n",
        "    \"clients\":\"client\",\n",
        "    \"product\":\"product\",\n",
        "    \"products\":\"product\",\n",
        "    \"produced\":\"produce\",\n",
        "    \"produce\":\"produce\",\n",
        "    \"producing\":\"produce\",\n",
        "    \"produces\": \"produce\",\n",
        "    \"labour\": \"labor\",\n",
        "    \"female\": \"female\",\n",
        "    \"women\": \"female\",\n",
        "    \"woman\": \"female\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3a0d96",
      "metadata": {
        "id": "ad3a0d96"
      },
      "outputs": [],
      "source": [
        "print(\"⏳ Running Preprocessing...\")\n",
        "\n",
        "df['clean_text'] = df[translate_column].apply(clean_text)\n",
        "df['lemma_text'] = df['clean_text'].apply(lemmatize_text)\n",
        "df['normalized_text'] = df['lemma_text'].apply(map_synonyms)\n",
        "print(\"✅ Preprocessing Complete.\")\n",
        "df[['clean_text', 'normalized_text']].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBTXpwkYAkeA"
      },
      "source": [
        "## 4. Semantic Vectorization (SBERT)\n",
        "We utilize **Sentence-BERT (SBERT)**, specifically the `all-MiniLM-L6-v2` model. Unlike TF-IDF, which relies on word overlap, SBERT generates dense vector representations (384 dimensions) where semantically similar ideas are mathematically close in vector space, regardless of the specific vocabulary used.\n",
        "\n",
        "**What is this step?**\n",
        "Traditional computer programs count words. Modern AI \"reads\" meaning. Here, we use a sophisticated AI model (SBERT) to convert every stakeholder idea into a \"location\" in a 384-dimensional map. In this map, ideas that mean the same thing (e.g., \"Higher pay\" and \"Better salaries\") are placed mathematically close together, even if they use different words.\n",
        "\n",
        "**Methodological Justification**\n",
        "We utilize Sentence-BERT (SBERT), specifically the all-MiniLM-L6-v2 architecture, to generate dense vector representations of the qualitative data. Unlike Bag-of-Words (BoW) models that rely on lexical overlap, SBERT uses Siamese networks to derive semantically meaningful sentence embeddings. This allows for the measurement of \"Semantic Distance\" using cosine similarity, capturing contextual nuances that keyword counting misses.\n",
        "\n",
        "**Reference:** Reimers, N. & Gurevych, I. (2019) Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks. *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing* and *the 9th International Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP)*, Hong Kong, November 2019, 3982-3992.\n",
        "https://doi.org/10.18653/v1/D19-1410"
      ],
      "id": "dBTXpwkYAkeA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPbI4FJCAkeA"
      },
      "outputs": [],
      "source": [
        "print(\"⏳ Encoding embeddings with Sentence-BERT...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the full sentences (not just keywords) for better context\n",
        "embeddings = model.encode(df['clean_text'].tolist(), show_progress_bar=True)\n",
        "print(f\"✅ Encoding Complete. Matrix Shape: {embeddings.shape}\")"
      ],
      "id": "vPbI4FJCAkeA"
    },
    {
      "cell_type": "markdown",
      "id": "eff8715d",
      "metadata": {
        "id": "eff8715d"
      },
      "source": [
        "## 5. Automatic Categorization\n",
        "For \"bertopic_labels\", we utilized the c-TF-IDF representation from the topic model to assign each unique idea to its dominant latent topic, providing a descriptive label derived from the most significant keywords in that cluster.\n",
        "For \"kmeans_labels\", a simple kmeans algorithm was employed.\n",
        "\n",
        "**What is this step?**\n",
        "In many rapid research scenarios, we do not have time to manually tag hundreds of ideas. This step acts as an \"Auto-Sorter.\" You can choose between two AI strategies to automatically group your data:\n",
        "\n",
        "\n",
        "1.   K-Means: Forces the data into a specific number of groups (e.g., \"Sort these ideas into exactly 4 piles\"). Best when you have a specific target structure in mind.\n",
        "2.   BERTopic: Lets the data speak for itself. The AI finds natural clusters and names them (e.g., \"I found 12 distinct topics, here is what they are\"). Best for discovery.\n",
        "\n",
        "**Methodological Justification**\n",
        "To facilitate rapid analysis in the absence of a priori coding, we implement a selectable unsupervised clustering pipeline. The framework offers two distinct algorithmic approaches:\n",
        "\n",
        "\n",
        "\n",
        "1.   K-Means Clustering: Partitions the embedding space into *`k`* distinct Voronoi cells, minimizing within-cluster variance. This is appropriate for verifying theoretical frameworks with a fixed number of dimensions.\n",
        "2.   BERTopic: A density-based clustering approach utilizing HDBSCAN and c-TF-IDF. This method is non-parametric (does not require pre-specifying *`k`*) and is superior for uncovering latent, irregular semantic structures inherent in the corpus.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. 1, 281–297. https://digitalassets.lib.berkeley.edu/math/ucb/text/math_s5_v1_article-17.pdf\n",
        "\n",
        "Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint https://doi.org/10.48550/arXiv.2203.05794."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5422e32",
      "metadata": {
        "id": "d5422e32"
      },
      "outputs": [],
      "source": [
        "def bertopic_modeling(clean_text_list, embeddings_bm, model_bm, topic_size):\n",
        "    representation_model = KeyBERTInspired()\n",
        "\n",
        "    topic_model_bm = BERTopic(\n",
        "        embedding_model=model_bm,\n",
        "        min_topic_size=topic_size,\n",
        "        representation_model=representation_model,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    topics_bm, probs_bm = topic_model_bm.fit_transform(clean_text_list, embeddings_bm)\n",
        "\n",
        "    return topic_model_bm, topics_bm, probs_bm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e42679",
      "metadata": {
        "id": "37e42679"
      },
      "outputs": [],
      "source": [
        "def kmeans_machine_labels(n_clusters, random_state=RANDOM_STATE, n_init=10):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n",
        "    labels_pred = kmeans.fit_predict(embeddings)\n",
        "    return labels_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3ff45e",
      "metadata": {
        "id": "ac3ff45e"
      },
      "outputs": [],
      "source": [
        "if MACHINE_CAT == \"kmeans_labels\":\n",
        "    machine_n_clusters = 4\n",
        "    df[MACHINE_CAT] = kmeans_machine_labels(n_clusters=machine_n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
        "elif MACHINE_CAT == \"bertopic_labels\":\n",
        "    topic_model, topics, probs = bertopic_modeling(clean_text_list=df['clean_text'].tolist(),\n",
        "                                                embeddings_bm=embeddings, # We reuse the 'embeddings' we already calculated to make this super fast\n",
        "                                                model_bm=model, # Reuse the SBERT model from Section 4\n",
        "                                                topic_size=5 # Allow small, granular topics\n",
        "                                                )\n",
        "    doc_info = topic_model.get_document_info(df['clean_text'].tolist())\n",
        "    df[MACHINE_CAT] = doc_info['Name'].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_results_to_sheets(spreadsheet_id=spreadsheet_id, processed_data=df.dropna(), new_worksheet_name=\"processing-labels\")"
      ],
      "metadata": {
        "id": "lwlMhYUvIYft"
      },
      "id": "lwlMhYUvIYft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOnnZC1lAkeB"
      },
      "source": [
        "## 6. Visualizing the Semantic Topology (UMAP)\n",
        "Here we project the 384-dimensional semantic space into 2D using **Uniform Manifold Approximation and Projection (UMAP)**. We visualize this projection under different parameter settings to inspect local vs. global structure.\n",
        "\n",
        "\n",
        "**What is this step?**\n",
        "\n",
        "Since humans cannot visualize 384 dimensions of meaning, we use UMAP to project the data onto a 2D map. Think of this as a \"GPS for Ideas.\" It creates a scatterplot where we can visually inspect the landscape of the discussion. We look for \"islands\" (distinct concepts), \"bridges\" (connecting themes), and \"outliers\" (unique ideas) to see how stakeholder inputs naturally group together.\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "To inspect the local and global structure of the high-dimensional data, we employ Uniform Manifold Approximation and Projection (UMAP). UMAP constructs a high-dimensional fuzzy topological representation of the data and optimizes a low-dimensional graph to maintain structural fidelity. This projection serves as a visual validation tool, allowing us to empirically observe the separation or overlap of the generated categories in a non-linear manifold.\n",
        "\n",
        "**Reference:** McInnes, L., Healy, J., & Melville, J. (2020). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv preprint https://doi.org/10.48550/arXiv.1802.03426."
      ],
      "id": "dOnnZC1lAkeB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et_PMM_pAkeB"
      },
      "outputs": [],
      "source": [
        "def plot_umap_grid(labels_col, title_prefix, palette='tab20'):\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle(f'UMAP Projections Colored by {title_prefix}', fontsize=16)\n",
        "\n",
        "    # (n_neighbors, min_dist, ax_coords, subtitle)\n",
        "    params = [\n",
        "        (5, 0.1, (0, 0), 'Local Focus (n=5, d=0.1)'),\n",
        "        (50, 0.1, (0, 1), 'Global Focus (n=50, d=0.1)'),\n",
        "        (15, 0.0, (1, 0), 'Tight Packing (n=15, d=0.0)'),\n",
        "        (15, 0.5, (1, 1), 'Spread Out (n=15, d=0.5)')\n",
        "    ]\n",
        "\n",
        "    handles, labels = [], []\n",
        "\n",
        "    for i, (n, d, coords, title) in enumerate(params):\n",
        "        ax = axs[coords]\n",
        "        reducer = umap.UMAP(n_neighbors=n, min_dist=d, n_components=2, metric='cosine', random_state=RANDOM_STATE)\n",
        "        embedding = reducer.fit_transform(embeddings)\n",
        "\n",
        "        temp_df = pd.DataFrame(embedding, columns=['x', 'y'])\n",
        "        temp_df['label'] = df[labels_col].values\n",
        "\n",
        "        if i == 0:\n",
        "            sns.scatterplot(x='x', y='y', hue='label', data=temp_df, palette=palette, s=40, ax=ax, legend='full')\n",
        "            handles, labels = ax.get_legend_handles_labels()\n",
        "            ax.get_legend().remove()\n",
        "        else:\n",
        "            sns.scatterplot(x='x', y='y', hue='label', data=temp_df, palette=palette, s=40, ax=ax, legend=False)\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    if handles:\n",
        "        fig.legend(handles, labels, loc='lower center', ncol=5, bbox_to_anchor=(0.5, 0.02))\n",
        "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
        "    plt.show()"
      ],
      "id": "et_PMM_pAkeB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bf00fd",
      "metadata": {
        "id": "08bf00fd"
      },
      "outputs": [],
      "source": [
        "# Plot Sub-Criteria (Granular)\n",
        "if RUN_SUB_CAT:\n",
        "    plot_umap_grid(SUBCRITERIA_COL, \"Sub-Criteria\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPmCYqHVAkeB"
      },
      "outputs": [],
      "source": [
        "# Plot Main Criteria (High Level)\n",
        "if RUN_MANUAL_CAT:\n",
        "    plot_umap_grid(CRITERIA_COL, \"Main Criteria\")"
      ],
      "id": "CPmCYqHVAkeB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f511fa4f",
      "metadata": {
        "id": "f511fa4f"
      },
      "outputs": [],
      "source": [
        "# If no manual categories, plot machine categories\n",
        "if not RUN_MANUAL_CAT and not RUN_SUB_CAT:\n",
        "    plot_umap_grid(MACHINE_CAT, \"Machine Categories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIi0GeNvAkeB"
      },
      "source": [
        "## 7. Machine vs. Human Clustering Validation\n",
        "To validate the distinctiveness of the COLOR framework, we compare the manual coding against unsupervised K-Means clustering.\n",
        "\n",
        "**What is this step?**\n",
        "\n",
        "Here we ask a critical question: Does the AI agree with our manual coding? We force the AI to group the ideas into clusters (K-Means) based purely on language. We then compare these robot-made clusters with the human-made categories.\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "We benchmark the manual qualitative coding against unsupervised K-Means Clustering. Using metrics such as the Silhouette Score (cluster separation) and Adjusted Mutual Information (AMI), we quantify the divergence between semantic topics (Machine) and normative constructs (Human).\n",
        "\n",
        "**Reference:** Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. *Journal of Computational and Applied Mathematics*, 20, 53-65. https://doi.org/10.1016/0377-0427(87)90125-7"
      ],
      "id": "xIi0GeNvAkeB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaWhTL5QAkeC"
      },
      "outputs": [],
      "source": [
        "def compare_clusters(n_clusters, manual_col, title):\n",
        "    # K-Means Clustering\n",
        "    labels_pred = kmeans_machine_labels(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
        "\n",
        "    # Metrics\n",
        "    labels_true = df[manual_col]\n",
        "    sil_machine = silhouette_score(embeddings, labels_pred, metric='cosine')\n",
        "    sil_human = silhouette_score(embeddings, labels_true, metric='cosine')\n",
        "    homogeneity = homogeneity_score(labels_true, labels_pred)\n",
        "    completeness = completeness_score(labels_true, labels_pred)\n",
        "    ami = adjusted_mutual_info_score(labels_true, labels_pred)\n",
        "\n",
        "    print(f\"--- {title} Metrics ---\")\n",
        "    print(f\"Machine (K-Means) Silhouette: {sil_machine:.4f}\")\n",
        "    print(f\"Human (Manual) Silhouette:    {sil_human:.4f}\")\n",
        "    print(f\"Homogeneity:                  {homogeneity:.4f}\")\n",
        "    print(f\"Completeness:                 {completeness:.4f}\")\n",
        "    print(f\"Adjusted Mutual Info (AMI):   {ami:.4f}\")\n",
        "    print(\"-\"*30)\n",
        "\n",
        "    df[f\"labels_pred_n{n_clusters}\"] = labels_pred\n",
        "    plot_umap_grid(f\"labels_pred_n{n_clusters}\", f'UMAP Colored by K-Means Clusters (k={n_clusters})')"
      ],
      "id": "oaWhTL5QAkeC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec717ff3",
      "metadata": {
        "id": "ec717ff3"
      },
      "outputs": [],
      "source": [
        "# Compare Sub-Criteria (K=16)\n",
        "if RUN_SUB_CAT:\n",
        "    compare_clusters(n_clusters=16, manual_col=SUBCRITERIA_COL, title=\"Sub-Criteria (K=16)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6a1c7a",
      "metadata": {
        "id": "af6a1c7a"
      },
      "outputs": [],
      "source": [
        "# Compare Main Criteria (K=4)\n",
        "if RUN_MANUAL_CAT:\n",
        "    compare_clusters(n_clusters=4, manual_col=CRITERIA_COL, title=\"Main Criteria (K=4)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTGYZWgIAkeC"
      },
      "source": [
        "## 8. Centroid Similarity Analysis\n",
        "This heatmap analysis reveals the \"Conceptual Distance\" between your criteria.\n",
        "*   **Yellow/Green:** High semantic overlap (vocabulary is similar).\n",
        "*   **Purple/Blue:** Low overlap (concepts are distinct).\n",
        "\n",
        "**What is this step?**\n",
        "\n",
        "This step measures the \"Conceptual Distance\" between your categories. It asks: Are these two groups actually different?\n",
        "\n",
        "*   High Similarity (Yellow/Green): The groups talk about the same things using the same words (e.g., \"Wages\" and \"Salaries\").\n",
        "*   Low Similarity (Purple/Blue): The groups are conceptually distinct (e.g., \"Pollution\" vs. \"Governance\"). This proves your categories aren't repetitive.\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "To quantify the distinctness of the identified clusters, we compute the Cosine Similarity between their centroids (mean vectors). By analyzing the orthogonality of these centroids in vector space, we can mathematically validate the discriminant validity of the taxonomy. High cosine similarity (≈1.0) suggests redundancy between categories, while lower scores indicate that the categories occupy distinct semantic niches.\n",
        "\n",
        "**Reference:** Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic indexing. *Communications of the ACM*, 18(11), 613-620. https://doi.org/10.1145/361219.361220"
      ],
      "id": "kTGYZWgIAkeC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWd1oIupAkeC"
      },
      "outputs": [],
      "source": [
        "def plot_centroid_heatmap(label_col, title):\n",
        "    unique_labels = sorted(df[label_col].dropna().unique())\n",
        "    centroids = []\n",
        "\n",
        "    for label in unique_labels:\n",
        "        indices = df[df[label_col] == label].index\n",
        "        group_vectors = embeddings[indices]\n",
        "        centroid = np.mean(group_vectors, axis=0)\n",
        "        centroids.append(centroid)\n",
        "\n",
        "    sim_matrix = cosine_similarity(np.array(centroids))\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(sim_matrix, xticklabels=unique_labels, yticklabels=unique_labels, cmap='viridis', annot=False)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "vWd1oIupAkeC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66d611f",
      "metadata": {
        "id": "e66d611f"
      },
      "outputs": [],
      "source": [
        "# Sub-Criteria Centroids\n",
        "if RUN_SUB_CAT:\n",
        "    plot_centroid_heatmap(SUBCRITERIA_COL, \"Cosine Similarity between Sub-Criteria Centroids\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff4742d",
      "metadata": {
        "id": "1ff4742d"
      },
      "outputs": [],
      "source": [
        "# Main Criteria Centroids\n",
        "if RUN_MANUAL_CAT:\n",
        "    plot_centroid_heatmap(CRITERIA_COL, \"Cosine Similarity between Main Criteria Centroids\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8470c204",
      "metadata": {
        "id": "8470c204"
      },
      "outputs": [],
      "source": [
        "# If no manual categories, plot machine categories\n",
        "if not RUN_MANUAL_CAT and not RUN_SUB_CAT:\n",
        "    plot_centroid_heatmap(MACHINE_CAT, \"Cosine Similarity between Machine Categories Centroids\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX3ZIXERAkeC"
      },
      "source": [
        "## 9. Keyword Co-occurrence Network (Louvain Community Detection)\n",
        "This analysis maps the semantic backbone of the dataset.\n",
        "*   **Nodes:** Lemmatized keywords.\n",
        "*   **Edges:** Co-occurrence frequency (only if they appear together $\\ge 3$ times).\n",
        "*   **Colors:** Semantic communities detected by the Louvain algorithm.\n",
        "\n",
        "\n",
        "**What is this step?**\n",
        "This creates a \"Brain Map\" of the dataset. It draws connections between words that frequently appear together (e.g., \"Water\" often appears with \"Pollution\"). By analyzing this web, we can find the concepts that tie everything together.\n",
        "\n",
        "**Methodological Justification**\n",
        "We construct a graph network where nodes represent lemmatized keywords and edges represent co-occurrence frequency (threshold $\\ge 3$). To identify latent thematic structures, we apply the Louvain Community Detection algorithm, which maximizes the modularity of the graph. This reveals the \"semantic backbone\" of the stakeholder discourse, identifying bridging concepts that link disparate operational domains.\n",
        "\n",
        "Reference: Blondel, V. D., Guillaume, J. L., Lambiotte, R., & Lefebvre, E. (2008). Fast unfolding of communities in large networks. *Journal of Statistical Mechanics: Theory and Experiment*, 2008(10), P10008. https://doi.org/10.48550/arXiv.0803.0476"
      ],
      "id": "wX3ZIXERAkeC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyI3pKfFAkeC"
      },
      "outputs": [],
      "source": [
        "def keyword_cooccurance_network(normalized_text, threshold, stop_words, random_state=RANDOM_STATE):\n",
        "\n",
        "    cv = CountVectorizer(ngram_range=(1,1), min_df=5, stop_words=stop_words)\n",
        "    X_cv = cv.fit_transform(normalized_text)\n",
        "    terms = cv.get_feature_names_out()\n",
        "\n",
        "    # Compute Co-occurrence Matrix\n",
        "    cooc = (X_cv.T @ X_cv).toarray()\n",
        "    np.fill_diagonal(cooc, 0)\n",
        "\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for i in range(len(terms)):\n",
        "        for j in range(i+1, len(terms)):\n",
        "            w = cooc[i, j]\n",
        "            if w >= threshold:\n",
        "                G.add_edge(terms[i], terms[j], weight=int(w))\n",
        "\n",
        "    if len(G.nodes) > 0:\n",
        "        # Louvain Detection\n",
        "        partition = community_louvain.best_partition(G, random_state=random_state)\n",
        "\n",
        "        plt.figure(figsize=(18, 14))\n",
        "        pos = nx.spring_layout(G, k=0.15, seed=random_state)\n",
        "\n",
        "        node_sizes = [v * 50 for v in dict(G.degree()).values()]\n",
        "        node_colors = [partition.get(node) for node in G.nodes()]\n",
        "\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, cmap=plt.cm.Pastel2_r, alpha=0.9)\n",
        "        nx.draw_networkx_edges(G, pos, alpha=0.2, edge_color='gray')\n",
        "        nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif', font_weight='light')\n",
        "\n",
        "        plt.title('Keyword Co-occurrence Network (Louvain Communities)', fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "id": "zyI3pKfFAkeC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8061038",
      "metadata": {
        "id": "a8061038"
      },
      "outputs": [],
      "source": [
        "custom_stops = list(ENGLISH_STOP_WORDS)\n",
        "\n",
        "keyword_cooccurance_network(df['normalized_text'], threshold=3, stop_words=custom_stops, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47959ce",
      "metadata": {
        "id": "b47959ce"
      },
      "source": [
        "## 10. Distinctive Keyword Extraction (c-TF-IDF)\n",
        "This analysis identifies the \"signature\" keywords for each category.\n",
        "It uses **Class-based TF-IDF**, which scores words based on how unique they are to a specific cluster relative to the rest of the dataset.\n",
        "\n",
        "**What is this step?**\n",
        "\n",
        "Simple frequency counts are often misleading because everyone uses generic words like \"company\" or \"textile.\" This step identifies the \"Signature Keywords\"—the words that make a specific group unique. It tells us not just what is being said, but what distinguishes Group A from Group B.\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "We employ Class-based TF-IDF (c-TF-IDF) to extract representative features for each cluster. Unlike standard TF-IDF which treats documents individually, c-TF-IDF aggregates all documents within a cluster into a single representation. It scores terms based on their frequency within the specific cluster relative to their ubiquity across the entire corpus, thereby filtering out generic stopwords and isolating the lexicon specific to each semantic category.\n",
        "\n",
        "**Reference:** Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint https://doi.org/10.48550/arXiv.2203.05794.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810652f3",
      "metadata": {
        "id": "810652f3"
      },
      "outputs": [],
      "source": [
        "def extract_ctfidf(df, label_col, text_col, n_keywords=10):\n",
        "\n",
        "    grouped = df.groupby(label_col)[text_col].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "    # max_df=0.7 means \"ignore words that appear in more than 70% of categories\" (removes generic stopwords)\n",
        "    tfidf = CountVectorizer(stop_words=custom_stops, max_df=0.7)\n",
        "    X = tfidf.fit_transform(grouped[text_col])\n",
        "    feature_names = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "    results = []\n",
        "    dense = X.todense()\n",
        "\n",
        "    for row_idx, row in enumerate(dense):\n",
        "        category = grouped.iloc[row_idx][label_col]\n",
        "        top_indices = np.asarray(row).argsort().flatten()[::-1][:n_keywords]\n",
        "\n",
        "        top_words = feature_names[top_indices]\n",
        "        top_scores = np.asarray(row).flatten()[top_indices]\n",
        "        keywords_str = \", \".join([f\"{word} ({score})\" for word, score in zip(top_words, top_scores)])\n",
        "\n",
        "        results.append({\n",
        "            \"Category\": category,\n",
        "            \"Distinctive Keywords (Frequency Score)\": keywords_str\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf74333",
      "metadata": {
        "id": "caf74333"
      },
      "outputs": [],
      "source": [
        "if RUN_MANUAL_CAT:\n",
        "    df_keywords_main = extract_ctfidf(df.dropna(), CRITERIA_COL, 'normalized_text', n_keywords=10)\n",
        "\n",
        "    print(\"\\n=== Distinctive Keywords for Main Criteria ===\")\n",
        "    display(df_keywords_main)\n",
        "    write_results_to_sheets(spreadsheet_id=spreadsheet_id, processed_data=df_keywords_main, new_worksheet_name=\"df_keywords_main\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09da2974",
      "metadata": {
        "id": "09da2974"
      },
      "outputs": [],
      "source": [
        "if RUN_SUB_CAT:\n",
        "    df_keywords_sub = extract_ctfidf(df.dropna(), SUBCRITERIA_COL, 'normalized_text', n_keywords=10)\n",
        "\n",
        "    print(\"\\n=== Distinctive Keywords for Sub-Criteria ===\")\n",
        "    display(df_keywords_sub)\n",
        "    write_results_to_sheets(spreadsheet_id=spreadsheet_id, processed_data=df_keywords_sub, new_worksheet_name=\"df_keywords_sub\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed1f73e",
      "metadata": {
        "id": "5ed1f73e"
      },
      "outputs": [],
      "source": [
        "# If no manual categories, plot machine categories\n",
        "if not RUN_MANUAL_CAT and not RUN_SUB_CAT:\n",
        "    df_keywords_mac = extract_ctfidf(df.dropna(), MACHINE_CAT, 'normalized_text', n_keywords=10)\n",
        "\n",
        "    print(\"\\n=== Distinctive Keywords for Machine Categories ===\")\n",
        "    display(df_keywords_mac)\n",
        "    write_results_to_sheets(spreadsheet_id=spreadsheet_id, processed_data=df_keywords_mac, new_worksheet_name=\"df_keywords_mac\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38fb6140",
      "metadata": {
        "id": "38fb6140"
      },
      "source": [
        "## 11. Advanced Topic Modeling with BERTopic\n",
        "\n",
        "**What is this step?**\n",
        "\n",
        "This tool automatically reads the text and summarizes the main conversations happening in the data (e.g., \"Topic 1: Wages,\" \"Topic 2: Recycling\"). This gives us a second, data-driven perspective on what participants are talking about.\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "We employ BERTopic, a dynamic topic modeling technique that leverages class-based TF-IDF (c-TF-IDF). Unlike LDA (Latent Dirichlet Allocation), which assumes a bag-of-words document structure, BERTopic utilizes the pre-trained semantic embeddings to form dense clusters and extracts coherent topic representations that preserve contextual meaning.\n",
        "\n",
        "**Reference:** Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint https://doi.org/10.48550/arXiv.2203.05794.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916dd272",
      "metadata": {
        "id": "916dd272"
      },
      "outputs": [],
      "source": [
        "topic_model, topics, probs = bertopic_modeling(clean_text_list=df['clean_text'].tolist(),\n",
        "                                                embeddings_bm=embeddings,\n",
        "                                                model_bm=model,\n",
        "                                                topic_size=7\n",
        "                                                )\n",
        "print(\"✅ Completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75523412",
      "metadata": {
        "id": "75523412"
      },
      "outputs": [],
      "source": [
        "freq = topic_model.get_topic_info()\n",
        "print(f\"✅ Found {len(freq) - 1} semantic topics.\") # -1 excludes the outlier topic (-1)\n",
        "display(freq.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c2f057",
      "metadata": {
        "id": "12c2f057"
      },
      "outputs": [],
      "source": [
        "if len(freq) > 2: # Check if there are more than 2 topics for meaningful visualization\n",
        "    fig_topics = topic_model.visualize_topics()\n",
        "    # fig_topics = topic_model.visualize_topics(topics=[-1,0,1,2,3,4])\n",
        "    fig_topics.show()\n",
        "else:\n",
        "    print(\"Not enough topics to visualize interdistance map effectively (found 2 or fewer topics). Change the topic_size parameter in bertopic_modeling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b1911ee",
      "metadata": {
        "id": "0b1911ee"
      },
      "outputs": [],
      "source": [
        "print(\"=== Topic Hierarchy ===\") # Shows how topics merge together\n",
        "fig_hierarchy = topic_model.visualize_hierarchy()\n",
        "fig_hierarchy.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f4aa35",
      "metadata": {
        "id": "55f4aa35"
      },
      "source": [
        "## 12. Efficient Few-Shot Classification (SetFit)\n",
        "\n",
        "**What is this step?**\n",
        "\n",
        "We train a \"Digital Assistant\" to learn your specific categorization logic. Using SetFit, we only need to show the AI about 8 examples per category. It uses these few examples to learn the difference between subtle concepts (like \"Internal Ethics\" vs. \"External Reputation\") much faster and more accurately than older methods that required thousands of examples.\n",
        "\n",
        "**Methodological Justification**\n",
        "\n",
        "To operationalize the coding scheme with minimal labeled data, we utilize SetFit (Sentence Transformer Fine-tuning). This approach employs Contrastive Learning on a pre-trained Sentence Transformer model. By generating positive and negative triplets from a small support set (n≈8 per class), the model fine-tunes its embedding space to maximize inter-class distance and minimize intra-class distance. This yields state-of-the-art performance for few-shot text classification tasks without the computational cost of Large Language Models (LLMs).\n",
        "\n",
        "**Reference:** Tunstall, L., et al. (2022). Efficient Few-Shot Learning Without Prompts. arXiv preprint https://doi.org/10.48550/arXiv.2209.11055."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0935831",
      "metadata": {
        "id": "a0935831"
      },
      "outputs": [],
      "source": [
        "SAMPLES_PER_CLASS = 8\n",
        "def few_shot_classification(COL_NAME, random_state=RANDOM_STATE):\n",
        "    train_df = df.groupby(COL_NAME).apply(lambda x: x.sample(n=min(len(x), SAMPLES_PER_CLASS), random_state=random_state)).reset_index(drop=True)\n",
        "    test_df = df.drop(train_df.index)\n",
        "\n",
        "    # Convert to HuggingFace Dataset format\n",
        "    # SetFit requires columns named 'text' and 'label'\n",
        "    train_ds = Dataset.from_pandas(train_df[[translate_column, COL_NAME]].rename(columns={translate_column: \"text\", COL_NAME: \"label\"}))\n",
        "    test_ds = Dataset.from_pandas(test_df[[translate_column, COL_NAME]].rename(columns={translate_column: \"text\", COL_NAME: \"label\"}))\n",
        "\n",
        "    print(f\"⏳ Training SetFit on just {len(train_df)} examples...\")\n",
        "\n",
        "    setfit_model_sf = SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    trainer_sf = Trainer(\n",
        "        model=setfit_model_sf,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=test_ds,\n",
        "        metric=\"accuracy\"\n",
        "    )\n",
        "\n",
        "    trainer_sf.train()\n",
        "    print(\"✅ Training Complete.\")\n",
        "    return trainer_sf, setfit_model_sf, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323f49b1",
      "metadata": {
        "id": "323f49b1"
      },
      "outputs": [],
      "source": [
        "def few_shot_visualization(setfit_model, test_ds):\n",
        "    preds = setfit_model.predict(test_ds[\"text\"])\n",
        "\n",
        "    # Create the Confusion Matrix\n",
        "    cm = confusion_matrix(test_ds[\"label\"], preds, labels=test_ds.unique(\"label\"))\n",
        "    labels = test_ds.unique(\"label\")\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "    plt.title('Confusion Matrix: Where did the model get confused?', fontsize=15)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7afe3bba",
      "metadata": {
        "id": "7afe3bba"
      },
      "outputs": [],
      "source": [
        "if RUN_MANUAL_CAT:\n",
        "  os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "  trainer_cr, setfit_model_cr, test_cr = few_shot_classification(CRITERIA_COL, random_state=RANDOM_STATE)\n",
        "  print(\"⏳ Evaluating on unseen test data...\")\n",
        "  metrics_cr = trainer_cr.evaluate()\n",
        "  print(f\"Accuracy: {metrics_cr['accuracy']:.4f}\")\n",
        "\n",
        "  few_shot_visualization(setfit_model_cr, test_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8b20d0",
      "metadata": {
        "id": "0e8b20d0"
      },
      "outputs": [],
      "source": [
        "if RUN_SUB_CAT:\n",
        "  os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "  trainer_sc, setfit_model_sc, test_sc  = few_shot_classification(SUBCRITERIA_COL, random_state=RANDOM_STATE)\n",
        "  print(\"⏳ Evaluating on unseen test data...\")\n",
        "  metrics_sc = trainer_sc.evaluate()\n",
        "  print(f\"Accuracy: {metrics_sc['accuracy']:.4f}\")\n",
        "\n",
        "  few_shot_visualization(setfit_model_sc, test_sc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3_B9Uk0LJDE"
      },
      "id": "Y3_B9Uk0LJDE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9b116462",
        "NwRy8_sWiG3D",
        "hBXj2-iBjTxq",
        "ARXaBFkKAkd_",
        "dBTXpwkYAkeA",
        "eff8715d",
        "dOnnZC1lAkeB",
        "xIi0GeNvAkeB",
        "kTGYZWgIAkeC",
        "wX3ZIXERAkeC",
        "b47959ce",
        "38fb6140",
        "55f4aa35"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
