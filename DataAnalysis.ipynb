{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcmvU1KMubRgt1PnVKCfqO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mberkancetin/Customizable-Structured-Electronic-Brainwriting/blob/main/DataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Overview and Welcome\n"
      ],
      "metadata": {
        "id": "qOTCbCpoZfAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** This Colab notebook is designed for academic and professional researchers to analyze textual data collected via Google Sheets from Customizable Structured Electronic Brainwriting (CSEB) session(s). It provides tools for automatic text classification (Zero-Shot and Few-Shot learning) and further exploratory analyses like Topic Modeling (LDA) and Correspondence Analysis.\n",
        "\n",
        "**Target Audience:** Researchers who need to categorize or understand themes within a corpus of text responses.\n",
        "\n",
        "**How to Use This Notebook:**\n",
        "1. Make a Copy (Recommended): To save your changes and outputs, go to \"File\" > \"Save a copy in Drive\".\n",
        "2. Follow the Steps: Execute the code cells in order from top to bottom.\n",
        "3. Provide Inputs: You will be prompted to provide your Google Sheet ID and make choices for certain analyses.\n",
        "4. Interpret Results: Explanations are provided to help you understand the outputs.\n",
        "5. Adapt (Optional): Feel free to adapt the code for your specific research needs.\n",
        "\n",
        "**Citation:**\n",
        "If you use this notebook or the underlying methodology in your research, please consider citing:\n"
      ],
      "metadata": {
        "id": "y1rqJkSgZq43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. SETUP: Environment and Google Sheet Connection\n",
        "\n",
        "### Please paste below the Google Sheet ID, sheet name, and column names provided by the Apps Script pop-up.\n"
      ],
      "metadata": {
        "id": "pspnkg-V-Oo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spreadsheet_id = \"PleaSe3nterY0ur-Spr34dshEetID\" #@param {type:\"string\"}\n",
        "sheet_name = \"PrepData\" #@param {type:\"string\"}\n",
        "original_column = \"RawIdea\" #@param {type:\"string\"}\n",
        "translate_column = \"Translation\" #@param {type:\"string\"}\n",
        "\n",
        "# If the original language of the session different than NLTK supported languages, keep the language as 'english'.\n",
        "# We will also show how to preprocess manually with other languages.\n",
        "# NLTK supported langugages: danish, dutch, finnish, french, german, greek, italian, portuguese, spanish, swedish\n",
        "# For zero-shot classifications, languages other than English should be found at: https://huggingface.co/models?pipeline_tag=zero-shot-classification\n",
        "\n",
        "languge = english # @param [\"english\", \"danish\", \"dutch\", \"finnish\", \"french\", \"german\", \"greek\", \"italian\", \"portuguese\", \"spanish\", \"swedish\"] {\"type\":\"raw\"}\n",
        "\n",
        "# Indicate category labels of data for zero-shot and few-shot classification methods\n",
        "category_labels = [\n",
        "    \"Exclusive CR Policies\",\n",
        "    \"Reactive CR Policies\",\n",
        "    \"Integrative CR Policies\",\n",
        "    \"Participative CR Policies\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "lbyxWdq1ZMi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. IMPORTS\n",
        "\n",
        "### This cell installs Python packages required for the analysis.\n",
        "(Self-correction: If any package installation requires a kernel restart, place that !pip install in its own cell right at the top, run it, let it restart, and then run subsequent cells. For most common libraries like pandas, gspread, scikit-learn, transformers, sentence-transformers, setfit, Colab often has them or handles updates gracefully. Explicitly state if a restart is common for a specific package)."
      ],
      "metadata": {
        "id": "2AJZgjNKB06s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to install required libraries.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "!pip install setfit\n",
        "from setfit import SetFitModel, Trainer\n",
        "\n",
        "!pip install prince\n",
        "import prince  # For correspondence analysis\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import string\n",
        "import re\n",
        "import urllib.request\n",
        "import pickle\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "!pip install nltk\n",
        "!pip install Levenshtein\n",
        "\n",
        "import nltk\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tag import PerceptronTagger, hmm, CRFTagger\n",
        "import Levenshtein\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "pIo-O3GkByDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. LOAD DATA: Authenticate and Connect to Google Sheets\n",
        "\n",
        "### Spreadsheet to Colab connection is done, data is transformed to be a pandas DataFrame. Another function, **write_results_to_sheets**, to write back to the spreadsheet is defined.\n",
        "\n",
        "### Before analysis, we'll select the appropriate text column (original or translated) and perform a basic exploration. Use **translated_df** DataFrame if ideas are translated, and if not, use **original_df** DataFrame."
      ],
      "metadata": {
        "id": "CTMI0-h1B0t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentication and setup\n",
        "def connect_to_sheets():\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "\n",
        "    # Connect to Google Sheets\n",
        "    gc = gspread.authorize(creds)\n",
        "    return gc\n",
        "\n",
        "# Load data from Google Sheets\n",
        "def load_ideas_from_sheets(spreadsheet_id, sheet_name):\n",
        "    gc = connect_to_sheets()\n",
        "    sh = gc.open_by_key(spreadsheet_id)\n",
        "    worksheet = sh.worksheet(sheet_name)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # Assuming the first row contains headers\n",
        "    headers = data[0]\n",
        "    ideas_data = data[1:]\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(ideas_data, columns=headers)\n",
        "    return df\n",
        "\n",
        "def write_results_to_sheets(spreadsheet_id, processed_data, new_worksheet_name):\n",
        "    \"\"\"\n",
        "    Write processed data back to a new worksheet in the same Google Sheets file.\n",
        "\n",
        "    Parameters:\n",
        "    - spreadsheet_id: The ID of the Google Sheets document\n",
        "    - original_sheet_name: Name of the original worksheet\n",
        "    - processed_data: DataFrame containing the processed results\n",
        "    - new_worksheet_name: Name of the new worksheet to create\n",
        "\n",
        "    Returns:\n",
        "    - The newly created worksheet object\n",
        "    write_results_to_sheets(spreadsheet_id, results_df, 'Processed_Results')\n",
        "    \"\"\"\n",
        "    gc = connect_to_sheets()\n",
        "\n",
        "    # Open the spreadsheet\n",
        "    sh = gc.open_by_key(spreadsheet_id)\n",
        "\n",
        "    # Check if worksheet already exists\n",
        "    try:\n",
        "        # If worksheet exists, delete it to avoid duplicates\n",
        "        existing_worksheet = sh.worksheet(new_worksheet_name)\n",
        "        sh.del_worksheet(existing_worksheet)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        # Worksheet doesn't exist, which is fine\n",
        "        pass\n",
        "\n",
        "    # Create a new worksheet\n",
        "    new_worksheet = sh.add_worksheet(title=new_worksheet_name, rows=processed_data.shape[0]+1, cols=processed_data.shape[1])\n",
        "\n",
        "    # Prepare the data for writing\n",
        "    # Convert DataFrame to list of lists, including headers\n",
        "    data_to_write = [processed_data.columns.tolist()] + processed_data.values.tolist()\n",
        "\n",
        "    # Write the data to the new worksheet\n",
        "    new_worksheet.update(range_name='A1', value=data_to_write)\n",
        "    print(f\"Successfully wrote results to new worksheet: {new_worksheet_name}\")\n",
        "\n",
        "    return new_worksheet\n",
        "\n",
        "\n",
        "# DataFrame is loaded with two columns\n",
        "df = load_ideas_from_sheets(spreadsheet_id, sheet_name)\n",
        "original_df = df[original_column]\n",
        "translated_df = df[translate_column]"
      ],
      "metadata": {
        "id": "wUmy1rxwmKtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 4. AUTOMATIC CLASSIFICATION METHODS\n",
        "\n",
        "### This section applies machine learning models to automatically classify your text data into predefined categories. You can choose between Zero-Shot Classification (no example data needed from you for training) or Few-Shot Classification (requires a small number of labeled examples).\n",
        "\n"
      ],
      "metadata": {
        "id": "fxQ6qpjpSMxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Define Category Labels for Classification\n",
        "\n",
        "### Regardless of the method chosen below, you need to define the categories (labels) you want to classify your text into."
      ],
      "metadata": {
        "id": "ykwX5Osyf27N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ⬇️ **Enter your classification labels, separated by commas:**\n",
        "#@markdown (e.g., \"Customer Complaint, Feature Request, Positive Feedback, Question\")\n",
        "CANDIDATE_LABELS_STR = \"Exclusive CR Policies, Reactive CR Policies, Integrative CR Policies, Participative CR Policies\" #@param {type:\"string\"}\n",
        "candidate_labels = [label.strip() for label in CANDIDATE_LABELS_STR.split(',')]\n",
        "\n",
        "if not candidate_labels or all(not lab for lab in candidate_labels):\n",
        "    print(\"⚠️ Please define your candidate labels above.\")\n",
        "else:\n",
        "    print(\"Using the following labels for classification:\")\n",
        "    for i, label in enumerate(candidate_labels):\n",
        "        print(f\"{i}: {label}\")"
      ],
      "metadata": {
        "id": "RH8qflT5faa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2 Method 1: Zero-Shot Classification**\n",
        "\n",
        "### Zero-shot classification allows you to classify text into labels the model hasn't explicitly been trained on, by leveraging large pre-trained language models. It's useful when you don't have labeled training data.\n",
        "### **Reference:**\n",
        "\n",
        "\n",
        "\n",
        "### This typically uses models like BART fine-tuned on Natural Language Inference (NLI). See Hugging Face Transformers documentation."
      ],
      "metadata": {
        "id": "LtVho4shfwqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ⬇️ **Select a Zero-Shot Classification model:**\n",
        "#@markdown (Common choices: 'facebook/bart-large-mnli', 'valhalla/distilbart-mnli-12-3', 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\n",
        "ZERO_SHOT_MODEL_NAME = \"facebook/bart-large-mnli\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ⬇️ **Run Zero-Shot Classification?**\n",
        "RUN_ZERO_SHOT = True #@param {type:\"boolean\"}\n",
        "\n",
        "def zero_shot_idea_classification(idea_column, candidate_labels, model):\n",
        "    \"\"\"\n",
        "    Classify session ideas into predefined categories\n",
        "    \"\"\"\n",
        "    # Zero-shot classification pipeline\n",
        "    classifier = pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    # Classification function\n",
        "    def classify_idea(idea):\n",
        "        result = classifier(idea, candidate_labels)\n",
        "        return result\n",
        "\n",
        "    # Create results DataFrame\n",
        "    return {\n",
        "        'result': [classify_idea(idea) for idea in idea_column]\n",
        "    }\n",
        "\n",
        "if RUN_ZERO_SHOT:\n",
        "    try:\n",
        "        org_results = zero_shot_idea_classification(original_df, category_labels, ZERO_SHOT_MODEL_NAME)\n",
        "        en_results = zero_shot_idea_classification(translated_df, category_labels, ZERO_SHOT_MODEL_NAME)\n",
        "        results = en_results\n",
        "\n",
        "        result_df = pd.DataFrame([\n",
        "            {'sequence': item['sequence'], **dict(zip(item['labels'], item['scores']))}\n",
        "            for item in results['result']\n",
        "        ])\n",
        "        result_df.iloc[:, 1:] = result_df.iloc[:, 1:].astype(float)\n",
        "        result_df[\"Topic\"] = result_df.iloc[:, 1:].apply(lambda row: row.idxmax(), axis=1)\n",
        "\n",
        "        print(\"Zero-Shot classification complete.\")\n",
        "\n",
        "        print(\"\\n--- Zero-Shot Classification Results ---\")\n",
        "        display(result_df)\n",
        "\n",
        "        print(\"\\n--- Label Distribution ---\")\n",
        "        display(result_df['labels'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%')\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.countplot(y='zero_shot_label', data=result_df, order=result_df['labels'].value_counts().index, palette=\"viridis\")\n",
        "        plt.title('Distribution of Zero-Shot Classified Labels')\n",
        "        plt.xlabel('Count')\n",
        "        plt.ylabel('Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Zero-Shot classification: {e}\")\n",
        "else:\n",
        "    print(\"Skipping Zero-Shot Classification.\")"
      ],
      "metadata": {
        "id": "ZMnCzYrfgTxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_idea_classification(idea_column, candidate_labels):\n",
        "    \"\"\"\n",
        "    Classify session ideas into predefined categories\n",
        "    \"\"\"\n",
        "    # Zero-shot classification pipeline\n",
        "    classifier = pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=\"facebook/bart-large-mnli\"\n",
        "    )\n",
        "\n",
        "    # Classification function\n",
        "    def classify_idea(idea):\n",
        "        result = classifier(idea, candidate_labels)\n",
        "        return result\n",
        "\n",
        "    # Create results DataFrame\n",
        "    return {\n",
        "        'result': [classify_idea(idea) for idea in idea_column]\n",
        "    }"
      ],
      "metadata": {
        "id": "yG-Jo_3icYOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_results = zero_shot_idea_classification(original_df, category_labels)\n",
        "en_results = zero_shot_idea_classification(translated_df, category_labels)\n",
        "results = en_results"
      ],
      "metadata": {
        "id": "159j4s2AVLxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame([\n",
        "    {'sequence': item['sequence'], **dict(zip(item['labels'], item['scores']))}\n",
        "    for item in results['result']\n",
        "])\n",
        "result_df.iloc[:, 1:] = result_df.iloc[:, 1:].astype(float)\n",
        "result_df[\"Topic\"] = result_df.iloc[:, 1:].apply(lambda row: row.idxmax(), axis=1)\n",
        "result_df"
      ],
      "metadata": {
        "id": "EQwAlvo0Sfng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_results_to_sheets(spreadsheet_id, result_df, \"zero_shot_classification\")"
      ],
      "metadata": {
        "id": "S0nwkwdSX4Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "\n",
        "# We select a subsample of 5000 abstracts from ArXiv\n",
        "dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"]\n",
        "docs = dataset[\"abstract\"][:5_000]\n",
        "\n",
        "# We define a number of topics that we know are in the documents\n",
        "zeroshot_topic_list = [\"Clustering\", \"Topic Modeling\", \"Large Language Models\"]\n",
        "\n",
        "# We fit our model using the zero-shot topics\n",
        "# and we define a minimum similarity. For each document,\n",
        "# if the similarity does not exceed that value, it will be used\n",
        "# for clustering instead.\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=\"thenlper/gte-small\",\n",
        "    min_topic_size=15,\n",
        "    zeroshot_topic_list=zeroshot_topic_list,\n",
        "    zeroshot_min_similarity=.85,\n",
        "    representation_model=KeyBERTInspired()\n",
        ")\n",
        "topics, _ = topic_model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "nb23oiFZgBg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3 Method 2: Few-Shot Classification**\n",
        "\n",
        "### SetFit allows for training a sentence transformer model with very few labeled examples per class. This can often yield better results than zero-shot if you can provide a small, high-quality set of examples.\n",
        "\n",
        "### **Reference:**\n",
        "Tunstall, L., et al. (2022). Efficient Few-Shot Learning Without Prompts. arXiv:2209.11055. https://doi.org/10.48550/arxiv.2209.11055\n",
        "\n",
        "Sentence-BERT: Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. EMNLP. http://arxiv.org/abs/1908.10084"
      ],
      "metadata": {
        "id": "cPwoijCMGMPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ⬇️ **Select a Sentence Transformer model for SetFit:**\n",
        "#@markdown (e.g., 'sentence-transformers/paraphrase-mpnet-base-v2', 'sentence-transformers/all-MiniLM-L6-v2', 'dbmdz/bert-base-turkish-cased' for Turkish)\n",
        "SETFIT_MODEL_NAME = \"sentence-transformers/paraphrase-mpnet-base-v2\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ⬇️ **Run Few-Shot Classification (SetFit)?**\n",
        "RUN_FEW_SHOT = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Provide a few examples (e.g., 4-16) for each category.\n",
        "few_shot_examples_df = pd.DataFrame({\n",
        "    'text': [\n",
        "                \"Companies should invest in projects that will support the local economy\",\n",
        "                \"Companies should prioritize workers' rights, health and safety\",\n",
        "                \"Consumers should be informed about recycling\",\n",
        "                \"Discrimination against workers exercising their right to organize should be prevented\",\n",
        "                \"Education and vocational skills development programs should be offered to local people\",\n",
        "                \"Fabrics must be long-lasting and durable, and they must be recycled\",\n",
        "                \"Flexible payment plans should be offered against price increases\",\n",
        "                \"Investments should be made in sustainable technology development processes\",\n",
        "                \"Occupational safety practices should be implemented at high standards\",\n",
        "                \"Overtime payments should be guaranteed and notice and compensation rights should be strengthened\",\n",
        "                \"Public relations activities should be carried out regularly regarding the ethical and social responsibilities of the company\",\n",
        "                \"The company must ensure transparency through audits and reporting\",\n",
        "                \"The company should design its social responsibility projects according to the reactions received\",\n",
        "                \"To ensure stakeholder participation, an anonymous complaint system should be established and free legal consultancy should be offered\",\n",
        "                \"Women's rights awareness projects should be implemented within the company\",\n",
        "                \"Workforce quality should be improved through training programs for employees\",\n",
        "              ],\n",
        "    'label': [\n",
        "                3, 0, 3, 1,\n",
        "                3, 2, 0, 2,\n",
        "                2, 0, 1, 2,\n",
        "                1, 3, 1, 0,\n",
        "              ]\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "Yjm7rh6Ib2om",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def few_shot_classification_with_setfit(idea_column, candidate_labels, few_shot_examples, model):\n",
        "    \"\"\"\n",
        "    Perform few-shot classification using SetFit with a classification head.\n",
        "\n",
        "    Parameters:\n",
        "    - idea_column: Series of ideas to classify\n",
        "    - candidate_labels: List of possible classification labels\n",
        "    - few_shot_examples: DataFrame containing 'text' and 'label' columns\n",
        "    - model: Hugging Face model for classification (SetFit model)\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with classification results including predicted label and confidence\n",
        "    \"\"\"\n",
        "    # Load the pre-trained SetFit model\n",
        "    model = SetFitModel.from_pretrained(model)\n",
        "\n",
        "    # Prepare the few-shot examples for training\n",
        "    X_train = few_shot_examples['text'].tolist()\n",
        "    y_train = few_shot_examples['label'].astype(int).tolist()\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "    # Convert to Hugging Face datasets\n",
        "    from datasets import Dataset\n",
        "\n",
        "    train_data = {'text': X_train, 'label': y_train}\n",
        "    val_data = {'text': X_val, 'label': y_val}\n",
        "\n",
        "    train_dataset = Dataset.from_dict(train_data)\n",
        "    eval_dataset = Dataset.from_dict(val_data)\n",
        "\n",
        "    # Set up the Trainer with the model, training dataset, and validation dataset\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Classify each idea in the column\n",
        "    results = []\n",
        "    for idea in idea_column:\n",
        "        # Perform the classification using the trained model\n",
        "        result = model.predict_proba([idea])\n",
        "\n",
        "        # Apply softmax to get probabilities (confidence scores)\n",
        "        probs = torch.nn.functional.softmax(result, dim=-1)\n",
        "\n",
        "        # Get the predicted labels (index of the max probability)\n",
        "        predictions = np.argmax(probs.numpy(), axis=1)\n",
        "        predictions = predictions.apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) and len(x) > 0 else x)\n",
        "\n",
        "        # Get the confidence scores for the top label\n",
        "        confidence_scores = np.max(probs.numpy(), axis=1)\n",
        "        confidence_scores = confidence_scores.apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) and len(x) > 0 else x)\n",
        "\n",
        "        results.append({\n",
        "            'idea': idea,\n",
        "            'predicted_label': predictions,\n",
        "            'confidence': confidence_scores\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    return df_results\n",
        "\n",
        "def visualize_classification_results(df_results):\n",
        "    \"\"\"\n",
        "    Create interactive visualizations for classification results\n",
        "\n",
        "    Parameters:\n",
        "    - df_results: DataFrame with classification results\n",
        "\n",
        "    Returns:\n",
        "    - Plotly figure objects\n",
        "    \"\"\"\n",
        "    # Scatter Plot with TSNE dimensionality reduction\n",
        "    # Convert categorical labels to numeric for visualization\n",
        "    df_results['label_code'] = pd.Categorical(df_results['predicted_label']).codes\n",
        "\n",
        "    # Perform TSNE to reduce dimensionality\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_results = tsne.fit_transform(\n",
        "        pd.get_dummies(df_results[['label_code', 'confidence']])\n",
        "    )\n",
        "\n",
        "    df_results['tsne_x'] = tsne_results[:, 0]\n",
        "    df_results['tsne_y'] = tsne_results[:, 1]\n",
        "\n",
        "    # Scatter Plot\n",
        "    scatter_fig = px.scatter(\n",
        "        df_results,\n",
        "        x='tsne_x',\n",
        "        y='tsne_y',\n",
        "        color='predicted_label',\n",
        "        size='confidence',\n",
        "        hover_data=['idea', 'predicted_label', 'confidence'],\n",
        "        title='Idea Classification Visualization',\n",
        "        labels={'tsne_x': 'Dimension 1', 'tsne_y': 'Dimension 2'}\n",
        "    )\n",
        "\n",
        "    # Pie Chart of Label Distribution\n",
        "    label_counts = df_results['predicted_label'].value_counts()\n",
        "    pie_fig = px.pie(\n",
        "        values=label_counts.values,\n",
        "        names=label_counts.index,\n",
        "        title='Classification Label Distribution'\n",
        "    )\n",
        "\n",
        "    # Confidence Box Plot\n",
        "    box_fig = px.box(\n",
        "        df_results,\n",
        "        x='predicted_label',\n",
        "        y='confidence',\n",
        "        title='Confidence Levels by Label'\n",
        "    )\n",
        "\n",
        "    return scatter_fig, pie_fig, box_fig"
      ],
      "metadata": {
        "id": "Ip_QgEukbJ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_FEW_SHOT:\n",
        "    try:\n",
        "        df_results = few_shot_classification_with_setfit(translated_df, category_labels, few_shot_examples_df, SETFIT_MODEL_NAME)\n",
        "\n",
        "        print(\"Few-Shot classification complete.\")\n",
        "\n",
        "        print(\"\\n--- Few-Shot Classification Results ---\")\n",
        "        display(result_df)\n",
        "\n",
        "        print(\"\\n--- Label Distribution ---\")\n",
        "        display(result_df['labels'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%')\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.countplot(y='zero_shot_label', data=result_df, order=result_df['labels'].value_counts().index, palette=\"viridis\")\n",
        "        plt.title('Distribution of Zero-Shot Classified Labels')\n",
        "        plt.xlabel('Count')\n",
        "        plt.ylabel('Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Few-Shot classification: {e}\")\n",
        "else:\n",
        "    print(\"Skipping Few-Shot Classification.\")\n"
      ],
      "metadata": {
        "id": "ObeaMx8KCe8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the 'predicted_label' and 'confidence' columns\n",
        "df_results[\"predicted_label\"] = df_results[\"predicted_label\"].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) and len(x) > 0 else x)\n",
        "df_results[\"confidence\"] = df_results[\"confidence\"].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) and len(x) > 0 else x)\n",
        "\n",
        "# Now write to Google Sheets\n",
        "write_results_to_sheets(spreadsheet_id, df_results, \"few_shot_classification2\")"
      ],
      "metadata": {
        "id": "2kLpnRZuFnIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results\n",
        "scatter_fig, pie_fig, box_fig = visualize_classification_results(df_results)\n",
        "\n",
        "scatter_fig.show()"
      ],
      "metadata": {
        "id": "ZFaaPJ4TFvI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_few_shot_examples_df = pd.DataFrame({\n",
        "    'text': [\n",
        "                \"Firmalar yerel ekonomiyi destekleyecek projelere yatırım yapmalı\",\n",
        "                \"Şirketler işçi haklarına, sağlığına ve güvenliğine öncelik vermeli\",\n",
        "                \"Tüketiciler geri dönüşüm konusunda bilgilendirilmeli\",\n",
        "                \"Örgütlenme hakkını kullanan işçilere yönelik ayrımcılık önlenmelidir\",\n",
        "                \"Yerel halka eğitim ve mesleki beceri geliştirme programları sunulmalı\",\n",
        "                \"Kumaşların uzun ömürlü ve dayanıklı olması ve geri dönüştürülmesi gerekiyor\",\n",
        "                \"Fiyat artışlarına karşı esnek ödeme planları sunulmalı\",\n",
        "                \"Sürdürülebilir teknoloji geliştirme süreçlerine yatırım yapılmalı\",\n",
        "                \"İş güvenliği uygulamaları yüksek standartlarda hayata geçirilmeli\",\n",
        "                \"Fazla mesai ödemeleri garanti altına alınmalı, ihbar ve tazminat hakları güçlendirilmeli\",\n",
        "                \"Şirketin etik ve sosyal sorumluluklarını gözeterek halkla ilişkiler faaliyetleri düzenli olarak yürütülmelidir\",\n",
        "                \"Şirket denetim ve raporlama yoluyla şeffaflığı sağlamalıdır\",\n",
        "                \"Firma sosyal sorumluluk projelerini gelen tepkilere göre tasarlamalı\",\n",
        "                \"Paydaş katılımının sağlanması için isimsiz şikayet sistemi kurulmalı ve ücretsiz hukuki danışmanlık sunulmalıdır\",\n",
        "                \"Şirket içinde kadın haklarına yönelik farkındalık projeleri hayata geçirilmeli\",\n",
        "                \"Çalışanlara yönelik eğitim programları ile iş gücü kalitesi artırılmalıdır\"\n",
        "              ],\n",
        "    'label': [\n",
        "                3, 0, 3, 1,\n",
        "                3, 2, 0, 2,\n",
        "                2, 0, 1, 2,\n",
        "                1, 3, 1, 0,\n",
        "              ]\n",
        "})\n"
      ],
      "metadata": {
        "id": "cVFfR9WMbAvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_df_results = few_shot_classification_with_setfit(original_df, category_labels, load_ideas_from_sheets(spreadsheet_id, \"Sheet2\"), \"dbmdz/bert-base-turkish-cased\")\n",
        "\n",
        "org_df_results"
      ],
      "metadata": {
        "id": "5mAWqievbPk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now write to Google Sheets\n",
        "# write_results_to_sheets(spreadsheet_id, org_df_results, \"few_shot_classification_org2\")"
      ],
      "metadata": {
        "id": "HRVT_YAHovzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results\n",
        "scatter_fig, pie_fig, box_fig = visualize_classification_results(org_df_results)\n",
        "\n",
        "# In Colab, you would display these:\n",
        "scatter_fig.show()\n",
        "pie_fig.show()\n",
        "box_fig.show()"
      ],
      "metadata": {
        "id": "u4dc8PcKbzhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. FURTHER ANALYSIS AFTER CLASSIFICATION\n",
        "\n",
        "### After classifying your data, you might want to perform further analyses like topic modeling on the text within specific categories, or correspondence analysis to see relationships between categories and other variables.\n",
        "### **Note:** These analyses often benefit from text preprocessing like tokenization, lemmatization, and stop-word removal.\n",
        "\n",
        "## **6.1 Text Preprocessing for Advanced Analysis**\n",
        "\n",
        "### This step prepares the text for models like LDA, BIRCH, Multidimensional Scaling, TF-IDF Analysis, and Correspondence Analysis. It involves tokenization, lowercasing, removing punctuation and stop words, and lemmatization."
      ],
      "metadata": {
        "id": "DNBBw0k-Alj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing English text"
      ],
      "metadata": {
        "id": "LVnNvNy2-FK-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L_gnCX2_v7q"
      },
      "outputs": [],
      "source": [
        "# Enhanced synonym and topic mapping\n",
        "synonym_groups = {\n",
        "    'audit': ['inspection', 'examination'],\n",
        "    'standard': ['guideline', 'protocol', 'benchmark'],\n",
        "    'report': ['document', 'summary', 'analysis'],\n",
        "    'comply': ['adhere', 'follow', 'conform'],\n",
        "    'labor': ['workforce', 'employment', 'work'],\n",
        "    'worker': ['employee', 'employees', 'workers', 'collar'],\n",
        "    'environmental': ['ecology', 'sustainability', 'green'],\n",
        "    'salary': ['wage', 'pay', 'wages', 'payment']\n",
        "}\n",
        "\n",
        "# Additional stopwords\n",
        "additional_stops = {\n",
        "    'employee', 'work', 'make', 'take', 'provide', 'instead',\n",
        "    'use', 'give', 'without', 'right',\n",
        "    'increase', 'prevent', 'support', 'area'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(text_series):\n",
        "    # Convert to lowercase and remove whitespaces\n",
        "    processed = text_series.str.strip().str.lower()\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    processed = processed.str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    processed = processed.str.replace(r'\\d+', '', regex=True)\n",
        "\n",
        "    # Prepare stopwords and lemmatizer\n",
        "    stop_words = set(stopwords.words(languge))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess_single_text(text):\n",
        "        # Split into words\n",
        "        words = text.split()\n",
        "\n",
        "        # Remove stopwords\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "\n",
        "        # Replace synonyms with canonical terms\n",
        "        transformed_words = []\n",
        "        for word in words:\n",
        "            replaced = False\n",
        "            for canonical, synonyms in synonym_groups.items():\n",
        "                if word in synonyms or word == canonical:\n",
        "                    transformed_words.append(canonical)\n",
        "                    replaced = True\n",
        "                    break\n",
        "            if not replaced and word not in additional_stops:\n",
        "                transformed_words.append(word)\n",
        "\n",
        "        # Lemmatize words (first as verbs, then as nouns)\n",
        "        lemmatized_words = [\n",
        "            lemmatizer.lemmatize(word, pos='v')\n",
        "            for word in transformed_words\n",
        "        ]\n",
        "\n",
        "        lemmatized_words = [\n",
        "            lemmatizer.lemmatize(word, pos='n')\n",
        "            for word in lemmatized_words\n",
        "        ]\n",
        "\n",
        "        # Join words back into a string\n",
        "        return ' '.join(lemmatized_words)\n",
        "\n",
        "    # Apply preprocessing to each text in the series\n",
        "    processed = processed.apply(preprocess_single_text)\n",
        "\n",
        "    return processed\n",
        "\n",
        "en_processed = preprocess_text(en_ideas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.2 Latent Dirichlet Allocation (LDA) Topic Modeling\n",
        "\n",
        "### LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. It's commonly used to discover abstract \"topics\" that occur in a collection of documents."
      ],
      "metadata": {
        "id": "ivhZjZZ24fY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ---\n",
        "#@markdown ⬇️ **Run LDA Topic Modeling?**\n",
        "RUN_LDA = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Main analysis function\n",
        "def analyze_ideas_lda(raw_ideas, processed_ideas, n_topics=4):\n",
        "        vectorizer = CountVectorizer(\n",
        "        max_df=0.8,      # Reduced to allow more diverse terms\n",
        "        min_df=3,        # Increased to filter out very rare terms\n",
        "        max_features=1500,  # Increased feature space\n",
        "        stop_words=list(additional_stops)\n",
        "    )\n",
        "    dtm = vectorizer.fit_transform(processed_ideas)\n",
        "\n",
        "    # Create and fit improved LDA model\n",
        "    lda_model = LatentDirichletAllocation(\n",
        "        n_components=n_topics,\n",
        "        random_state=42,\n",
        "        learning_method='online',\n",
        "        max_iter=20,         # Increased iterations\n",
        "        learning_offset=50., # Helps with online learning\n",
        "        learning_decay=0.7,  # More gradual learning\n",
        "        doc_topic_prior=None,  # Let model learn topic distributions\n",
        "        topic_word_prior=None  # Let model learn word distributions\n",
        "    )\n",
        "    lda_output = lda_model.fit_transform(dtm)\n",
        "\n",
        "    # Get feature names (words)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Topic-word distribution with improved display\n",
        "    def display_topics(model, feature_names, n_top_words):\n",
        "        topic_dict = {}\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            # Get top words with their weights\n",
        "            top_words_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
        "            topic_words = [\n",
        "                (feature_names[i], round(topic[i], 3))\n",
        "                for i in top_words_indices\n",
        "            ]\n",
        "            topic_dict[f\"Topic {topic_idx+1}\"] = topic_words\n",
        "        return topic_dict\n",
        "\n",
        "    # Display top words for each topic\n",
        "    topic_words = display_topics(lda_model, feature_names, 10)\n",
        "\n",
        "    # Document-topic distribution\n",
        "    doc_topic_df = pd.DataFrame(lda_output)\n",
        "    doc_topic_df.columns = [f'Topic {i+1}' for i in range(n_topics)]\n",
        "\n",
        "    # Add original ideas and dominant topic\n",
        "    doc_topic_df['Idea'] = raw_ideas.values\n",
        "    doc_topic_df['Dominant_Topic'] = doc_topic_df.iloc[:, :n_topics].idxmax(axis=1)\n",
        "\n",
        "    # Calculate frequency of each topic\n",
        "    topic_counts = doc_topic_df['Dominant_Topic'].value_counts().reset_index()\n",
        "    topic_counts.columns = ['Topic', 'Count']\n",
        "\n",
        "    # Return results\n",
        "    results = {\n",
        "        'topic_words': topic_words,\n",
        "        'doc_topic_distribution': doc_topic_df,\n",
        "        'topic_counts': topic_counts,\n",
        "        'raw_ideas': raw_ideas,\n",
        "        'processed_ideas': processed_ideas\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to analyze proximity to predefined topic groups\n",
        "def analyze_proximity_to_topics(doc_topic_df, predefined_topics):\n",
        "    \"\"\"\n",
        "    Analyze how LDA topics align with predefined topic groups\n",
        "\n",
        "    Parameters:\n",
        "    - doc_topic_df: DataFrame with document-topic distributions\n",
        "    - predefined_topics: Dictionary with topic names as keys and lists of keywords as values\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame showing alignment between LDA topics and predefined topics\n",
        "    \"\"\"\n",
        "    # Create a matrix to store similarity scores\n",
        "    similarity_matrix = np.zeros((len(predefined_topics), len(predefined_topics)))\n",
        "    # Get the top words for each LDA topic\n",
        "    lda_topic_words = list(doc_topic_df.iloc[:, :4].idxmax(axis=1).unique())\n",
        "\n",
        "    # For each predefined topic and each LDA topic, calculate similarity\n",
        "    for i, (topic_name, keywords) in enumerate(predefined_topics.items()):\n",
        "        for j, lda_topic in enumerate(range(4)):\n",
        "            # Calculate similarity score (e.g., based on keyword overlap)\n",
        "            # This is a simple example - you can use more sophisticated methods\n",
        "            topic_ideas = doc_topic_df[doc_topic_df.iloc[:, :4].idxmax(axis=1) == f'Topic {j+1}']['Idea']\n",
        "\n",
        "            # Count how many ideas in this topic contain the keywords\n",
        "            keyword_matches = 0\n",
        "            for idea in topic_ideas:\n",
        "                for keyword in keywords:\n",
        "                    if keyword.lower() in idea.lower():\n",
        "                        keyword_matches += 1\n",
        "                        break\n",
        "\n",
        "            if len(topic_ideas) > 0:\n",
        "                similarity_matrix[i, j] = keyword_matches / len(topic_ideas)\n",
        "            else:\n",
        "                similarity_matrix[i, j] = 0\n",
        "\n",
        "    # Create DataFrame for the similarity matrix\n",
        "    similarity_df = pd.DataFrame(similarity_matrix,\n",
        "                               index=predefined_topics.keys(),\n",
        "                               columns=[f'Topic {i+1}' for i in range(4)])\n",
        "\n",
        "    return similarity_df"
      ],
      "metadata": {
        "id": "-49jKIvKah8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_LDA:\n",
        "  # Run LDA analysis with tanslations\n",
        "  en_results = analyze_ideas_lda(en_ideas, en_processed, n_topics=4)\n",
        "\n",
        "  # Analyze proximity to predefined topics\n",
        "  ### proximity_df = analyze_proximity_to_topics(en_results['doc_topic_distribution'], org_predefined_topics)\n",
        "  proximity_df = analyze_proximity_to_topics(en_results['doc_topic_distribution'], en_predefined_topics)\n",
        "\n",
        "  # Display en_results\n",
        "  print(\"Top words for each LDA topic:\")\n",
        "  for topic, words in en_results['topic_words'].items():\n",
        "      print(f\"{topic}: {', '.join(words)}\")\n",
        "\n",
        "  print(\"\\nTopic distribution across ideas:\")\n",
        "  print(en_results['topic_counts'])\n",
        "\n",
        "  # Visualization of topic distribution\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.barplot(x='Topic', y='Count', data=en_results['topic_counts'])\n",
        "  plt.title('Frequency of Dominant Topics')\n",
        "  plt.xlabel('Topic')\n",
        "  plt.ylabel('Number of Ideas')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "else:\n",
        "  print(\"Skipping LDA Topic Modeling.\")"
      ],
      "metadata": {
        "id": "ek6BGOoI9h15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAlignment with predefined topic groups:\")\n",
        "print(proximity_df)\n",
        "\n",
        "# Visualize alignment\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(proximity_df, annot=True, cmap=\"YlGnBu\")\n",
        "plt.title('Alignment between LDA Topics and Predefined Topics')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C2aRMT-BrrWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_results[\"doc_topic_distribution\"]\n"
      ],
      "metadata": {
        "id": "5gq2tFZdhbxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3 Correspondence Analysis (CA)\n",
        "\n",
        "### Correspondence Analysis is a statistical technique that provides a graphical representation of the relationships between rows and columns of a contingency table. For example, you could explore the relationship between your classified labels (e.g., 'zero_shot_label') and another categorical variable in your dataset, or between LDA topics and another variable."
      ],
      "metadata": {
        "id": "vVyhMk_swyqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"en_processed\"] = en_processed\n",
        "correspondance_df = df.loc[:, [\"en_processed\", \"Category\"]]\n",
        "correspondance_df['Category'] = correspondance_df['Category'].astype(dtype=\"int8\")"
      ],
      "metadata": {
        "id": "MMOeiOaHya2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_correspondence_analysis(contingency_table):\n",
        "    \"\"\"\n",
        "    Perform correspondence analysis on a contingency table\n",
        "\n",
        "    Parameters:\n",
        "    - contingency_table: DataFrame with counts\n",
        "\n",
        "    Returns:\n",
        "    - ca: Correspondence analysis object\n",
        "    - statistics: Dictionary with CA statistics\n",
        "    \"\"\"\n",
        "    # Perform chi-square test\n",
        "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "    # Calculate total observations\n",
        "    n = contingency_table.values.sum()\n",
        "\n",
        "    # Calculate phi^2 (effect size)\n",
        "    phi2 = chi2 / n\n",
        "\n",
        "    # Create CA model\n",
        "    ca = prince.CA(\n",
        "        n_components=min(contingency_table.shape) - 1,\n",
        "        n_iter=3,\n",
        "        copy=True,\n",
        "        check_input=True,\n",
        "        engine='sklearn',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    ca_result = ca.fit(contingency_table)\n",
        "    # print(dir(ca_result))\n",
        "\n",
        "    # Get eigenvalues and explained inertia\n",
        "    eigenvalues = ca_result.eigenvalues_\n",
        "    explained_inertia = ca_result.percentage_of_variance_\n",
        "    cumulative_inertia = ca_result.cumulative_percentage_of_variance_\n",
        "\n",
        "    # Create eigenvalue summary table\n",
        "    eigenvalue_summary = pd.DataFrame({\n",
        "        'Dimension': [f'Dim {i+1}' for i in range(len(eigenvalues))],\n",
        "        'Eigenvalue': eigenvalues,\n",
        "        'Percentage of Inertia': explained_inertia,\n",
        "        'Cumulative Percentage': cumulative_inertia\n",
        "    })\n",
        "\n",
        "    # Calculate high phi2 threshold (Greenacre's recommendation)\n",
        "    high_phi2_threshold = 1/min(contingency_table.shape[0]-1, contingency_table.shape[1]-1)\n",
        "\n",
        "    # Collect statistics\n",
        "    statistics = {\n",
        "        'n': n,\n",
        "        'chi2': chi2,\n",
        "        'p_value': p,\n",
        "        'dof': dof,\n",
        "        'phi2': phi2,\n",
        "        'high_phi2_threshold': high_phi2_threshold,\n",
        "        'eigenvalue_summary': eigenvalue_summary\n",
        "    }\n",
        "\n",
        "    return ca_result, statistics\n",
        "\n",
        "def visualize_ca_results(ca_result, contingency_table, statistics):\n",
        "    \"\"\"\n",
        "    Create Plotly visualizations for correspondence analysis results\n",
        "\n",
        "    Parameters:\n",
        "    - ca_result: Fitted CA model\n",
        "    - contingency_table: Original contingency table\n",
        "    - statistics: Dictionary with CA statistics\n",
        "\n",
        "    Returns:\n",
        "    - figures: List of Plotly figures\n",
        "    \"\"\"\n",
        "    # Extract row and column coordinates\n",
        "    row_coords = ca_result.row_coordinates(contingency_table)\n",
        "    col_coords = ca_result.column_coordinates(contingency_table)\n",
        "\n",
        "    # Create symmetric biplot\n",
        "    fig_biplot = go.Figure()\n",
        "\n",
        "    # Add row points\n",
        "    fig_biplot.add_trace(go.Scatter(\n",
        "        x=row_coords.iloc[:, 0],\n",
        "        y=row_coords.iloc[:, 1],\n",
        "        mode='markers+text',\n",
        "        marker=dict(color='blue', size=10),\n",
        "        text=row_coords.index,\n",
        "        name='Row Points',\n",
        "        textposition='top center'\n",
        "    ))\n",
        "\n",
        "    # Add column points\n",
        "    fig_biplot.add_trace(go.Scatter(\n",
        "        x=col_coords.iloc[:, 0],\n",
        "        y=col_coords.iloc[:, 1],\n",
        "        mode='markers+text',\n",
        "        marker=dict(color='red', size=10, symbol='triangle-up'),\n",
        "        text=col_coords.index,\n",
        "        name='Column Points',\n",
        "        textposition='top center'\n",
        "    ))\n",
        "\n",
        "    # Update layout\n",
        "    fig_biplot.update_layout(\n",
        "        title='Correspondence Analysis Biplot',\n",
        "        xaxis_title=f'Dimension 1 ({statistics[\"eigenvalue_summary\"][\"Percentage of Inertia\"][0]:.2f}%)',\n",
        "        yaxis_title=f'Dimension 2 ({statistics[\"eigenvalue_summary\"][\"Percentage of Inertia\"][1]:.2f}%)',\n",
        "        xaxis=dict(zeroline=True),\n",
        "        yaxis=dict(zeroline=True),\n",
        "        legend=dict(\n",
        "            x=1.05,\n",
        "            y=1,\n",
        "            traceorder='normal'\n",
        "        ),\n",
        "        width=2100,  # Set the width of the plot\n",
        "        height=1000,  # Set the height of the plot\n",
        "        violingap=True\n",
        "    )\n",
        "\n",
        "    # Create symmetric biplot\n",
        "    fig_biplot2 = go.Figure()\n",
        "\n",
        "    # Add row points\n",
        "    fig_biplot2.add_trace(go.Scatter(\n",
        "        x=row_coords.iloc[:, 1],\n",
        "        y=row_coords.iloc[:, 2],\n",
        "        mode='markers+text',\n",
        "        marker=dict(color='blue', size=10),\n",
        "        text=row_coords.index,\n",
        "        name='Row Points',\n",
        "        textposition='top center'\n",
        "    ))\n",
        "\n",
        "    # Add column points\n",
        "    fig_biplot2.add_trace(go.Scatter(\n",
        "        x=col_coords.iloc[:, 1],\n",
        "        y=col_coords.iloc[:, 2],\n",
        "        mode='markers+text',\n",
        "        marker=dict(color='red', size=10, symbol='triangle-up'),\n",
        "        text=col_coords.index,\n",
        "        name='Column Points',\n",
        "        textposition='top center'\n",
        "    ))\n",
        "\n",
        "    # Update layout\n",
        "    fig_biplot2.update_layout(\n",
        "        title='Correspondence Analysis Biplot',\n",
        "        xaxis_title=f'Dimension 2 ({statistics[\"eigenvalue_summary\"][\"Percentage of Inertia\"][1]:.2f}%)',\n",
        "        yaxis_title=f'Dimension 3 ({statistics[\"eigenvalue_summary\"][\"Percentage of Inertia\"][2]:.2f}%)',\n",
        "        xaxis=dict(zeroline=True),\n",
        "        yaxis=dict(zeroline=True),\n",
        "        legend=dict(\n",
        "            x=1.05,\n",
        "            y=1,\n",
        "            traceorder='normal'\n",
        "        ),\n",
        "        width=2100,  # Set the width of the plot\n",
        "        height=1000,  # Set the height of the plot\n",
        "        violingap=True\n",
        "    )\n",
        "\n",
        "    # Add scree plot for eigenvalues\n",
        "    eigvals = statistics['eigenvalue_summary']\n",
        "\n",
        "    fig_scree = go.Figure()\n",
        "    fig_scree.add_trace(go.Bar(\n",
        "        x=eigvals['Dimension'],\n",
        "        y=eigvals['Percentage of Inertia'],\n",
        "        name='Percentage of Inertia'\n",
        "    ))\n",
        "\n",
        "    fig_scree.add_trace(go.Scatter(\n",
        "        x=eigvals['Dimension'],\n",
        "        y=eigvals['Cumulative Percentage'],\n",
        "        mode='lines+markers',\n",
        "        name='Cumulative Percentage'\n",
        "    ))\n",
        "\n",
        "    fig_scree.update_layout(\n",
        "        title='Scree Plot: Explained Inertia by Dimension',\n",
        "        xaxis_title='Dimensions',\n",
        "        yaxis_title='Percentage of Inertia',\n",
        "        yaxis=dict(ticksuffix='%'),\n",
        "        legend=dict(\n",
        "            x=0.01,\n",
        "            y=0.99,\n",
        "            bgcolor='rgba(255, 255, 255, 0.5)',\n",
        "            bordercolor='rgba(0, 0, 0, 0.5)'\n",
        "        ),\n",
        "        barmode='overlay',\n",
        "        width=2100,  # Set the width of the plot\n",
        "        height=1000,  # Set the height of the plot\n",
        "        violingap=True\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    return [fig_biplot, fig_biplot2, fig_scree]\n",
        "\n",
        "def print_ca_statistics(statistics):\n",
        "    \"\"\"Print detailed CA statistics\"\"\"\n",
        "    print(\"\\n=== CORRESPONDENCE ANALYSIS STATISTICS ===\\n\")\n",
        "\n",
        "    print(f\"Total observations (n): {statistics['n']}\")\n",
        "    print(f\"Chi-Square (X²): {statistics['chi2']:.4f}\")\n",
        "    print(f\"Degrees of freedom: {statistics['dof']}\")\n",
        "    print(f\"p-value: {statistics['p_value']:.6f}\")\n",
        "    print(f\"Effect size (phi²): {statistics['phi2']:.4f}\")\n",
        "    print(f\"High phi² threshold: {statistics['high_phi2_threshold']:.4f}\")\n",
        "\n",
        "    if statistics['phi2'] > statistics['high_phi2_threshold']:\n",
        "        print(\"Association strength: Strong (phi² > threshold)\")\n",
        "    else:\n",
        "        print(\"Association strength: Weak (phi² <= threshold)\")\n",
        "\n",
        "    print(\"\\n=== EIGENVALUES AND INERTIA ===\\n\")\n",
        "    print(statistics['eigenvalue_summary'].to_string(index=False))\n",
        "\n",
        "# Integrate CA with classification results\n",
        "def classify_and_analyze(dfs):\n",
        "    \"\"\"\n",
        "    Perform classification and correspondence analysis\n",
        "\n",
        "    Parameters:\n",
        "    - dfs: dataframe of contingency table\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary with results and visualizations\n",
        "    \"\"\"\n",
        "    # For demonstration, we'll use simulated data\n",
        "    # In a real scenario, replace this with actual classification\n",
        "    contingency_table = dfs\n",
        "\n",
        "    # Perform CA\n",
        "    ca_result, statistics = perform_correspondence_analysis(contingency_table)\n",
        "\n",
        "    # Create visualizations\n",
        "    figures = visualize_ca_results(ca_result, contingency_table, statistics)\n",
        "\n",
        "    # Print statistics\n",
        "    print_ca_statistics(statistics)\n",
        "\n",
        "    return {\n",
        "        'contingency_table': contingency_table,\n",
        "        'ca_result': ca_result,\n",
        "        'statistics': statistics,\n",
        "        'figures': figures\n",
        "    }"
      ],
      "metadata": {
        "id": "ffbEpTMF5Xsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correspondance_df"
      ],
      "metadata": {
        "id": "nlnDUxBnKQ6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(\n",
        "    stop_words=languge,\n",
        "    max_features=1000,\n",
        "    ngram_range=(1,2),  # Include bigrams\n",
        "    max_df=0.90,  # Ignore terms in more than 80% of documents\n",
        "    min_df=3     # Ignore terms that appear in less than 2 documents\n",
        ")\n",
        "\n",
        "# Compute TF-IDF vectors\n",
        "count_matrix = vectorizer.fit_transform(correspondance_df[\"en_processed\"])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Add word counts to a new dataframe\n",
        "word_counts = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Add categories to the word counts\n",
        "word_counts['Category'] = correspondance_df['Category']\n",
        "\n",
        "# Group by categories and sum word counts\n",
        "frequency_table = word_counts.groupby('Category').sum()\n",
        "\n",
        "# Replace the numerical index with new labels\n",
        "frequency_table.index = list(en_predefined_topics.keys())\n",
        "\n",
        "frequency_table"
      ],
      "metadata": {
        "id": "bBB6FEYJJdn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_results = classify_and_analyze(frequency_table)"
      ],
      "metadata": {
        "id": "PgtfNGvbQ4YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print_ca_statistics(corr_results['statistics'])"
      ],
      "metadata": {
        "id": "rJCuyYBDhW20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_results['figures'][0].show()  # Biplot\n",
        "corr_results['figures'][1].show()  # Biplot\n",
        "corr_results['figures'][2].show()  # Scree plot"
      ],
      "metadata": {
        "id": "PriD1awTVPNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_results['ca_result'].row_contributions_"
      ],
      "metadata": {
        "id": "zkMrL_X4dTQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_results['ca_result'].column_contributions_"
      ],
      "metadata": {
        "id": "2ns9Wh9TdggP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}